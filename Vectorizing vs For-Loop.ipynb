{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this quick entry is to show the difference in computation-time between a for loop and a vectorized dot-product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10000000)\n",
    "w = np.random.rand(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07586431503295898\n"
     ]
    }
   ],
   "source": [
    "y=0\n",
    "tic = time.time()\n",
    "y = np.dot(w,x)\n",
    "toc = time.time()\n",
    "\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.33963394165039\n"
     ]
    }
   ],
   "source": [
    "y=0\n",
    "tic = time.time()\n",
    "for i in range(len(x)):\n",
    "    y = y + (x[i] * w[i])\n",
    "toc = time.time()\n",
    "\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow... the for loop took almost 1000 times the time as the vectorized dot-product!!!\n",
    "\n",
    "Let's look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03924441337585449\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "y = np.exp(x)\n",
    "toc = time.time()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.307820320129395\n"
     ]
    }
   ],
   "source": [
    "y = np.zeros([len(x), 1])\n",
    "tic = time.time()\n",
    "for i in range(len(x)):\n",
    "    y[i] = math.exp(x[i])\n",
    "toc = time.time()\n",
    "\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, vectorizing speeds up the process by ALOT!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the logistic regression algorithm, and what it looks like vectorized vs unvectorized:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $m$ is the number of datapoints in $X$\n",
    "2. $n$ is the number of features each datapoint in $X$ has\n",
    "3. $X$ is an $mxn$ matrix. It's got $m$ rows and $n$ columns. Each row corresponds to an individual datapoint $\\vec{x}$, holding information about $n$ features each. It's randomly generated. \n",
    "4. $Y$ is a vector holding the correct labels for each of our $\\vec{x}_s$ - the $i_{th}$ entry in $\\vec{Y}$, which we'll call $y_i$, is the correct label for the $i_{th}$ row of $X$, which we'll call $\\vec{x}_i$. Each entry (row) in $Y$ is given by the sigmoid of the linear combination of the two features *(columns)* of each row of $X: 0.2C_0 - 0.3C_1$. In other words, $0.2 \\hat{i} - 0.3\\hat{j}$ is the correct weight vector that we hope our logistic regression algorithm will get to. $Y$ can only hold $1_s$ and $0_s$, *(we're assuming that the correct labels are binary)* which is what the next few lines are doing - turning the labels in $Y$ into $1_s$ and $0_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56851308 0.18675876]\n",
      " [0.03442909 0.25015737]\n",
      " [0.29808635 0.48096666]\n",
      " [0.25995842 0.83796735]\n",
      " [0.30176686 0.62806705]\n",
      " [0.39236232 0.24862946]\n",
      " [0.56048842 0.59142251]\n",
      " [0.02554223 0.3188979 ]\n",
      " [0.84551615 0.59478436]\n",
      " [0.89047053 0.59187915]]\n",
      "[1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "m = 10 \n",
    "n=2\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.dot(X, ([.2, -.3]))\n",
    "\n",
    "for val in range(len(Y)):\n",
    "    Y[val] = (1/(1 + math.exp(-Y[val])))\n",
    "Y = np.round(Y)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [.1, -.1]\n",
    "b = np.random.randint(-5, 5)\n",
    "E=0; db=0;\n",
    "dW = [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7942223761225804\n",
      "0.5837624408275794\n",
      "0.5249437099254616\n",
      "0.4811939372819247\n",
      "0.4473007752610365\n",
      "0.4201601285414925\n",
      "0.39784944448039355\n",
      "0.37912342843286606\n",
      "0.3631401817225821\n",
      "0.3493085714069628\n",
      "0.3371999346822178\n",
      "0.3264948904389487\n",
      "0.31695000877632223\n",
      "0.30837615027819\n",
      "0.30062392474136407\n",
      "0.29357364803783437\n",
      "0.28712823264466386\n",
      "0.2812080463554878\n",
      "0.2757471246991002\n",
      "0.2706903349542974\n",
      "0.2659912220061957\n",
      "0.2616103510540692\n",
      "0.2575140178164741\n",
      "0.2536732342085468\n",
      "0.25006292300928656\n",
      "0.24666127282366065\n",
      "0.24344921722628743\n",
      "0.24041001099979323\n",
      "0.2375288829396192\n",
      "0.23479274951834933\n",
      "0.23218997728470264\n",
      "0.22971018455995756\n",
      "0.22734407502955495\n",
      "0.22508329738158786\n",
      "0.22292032633999845\n",
      "0.22084836136788905\n",
      "0.21886124004077961\n",
      "0.2169533636591942\n",
      "0.21511963312057353\n",
      "0.2133553934291718\n",
      "0.2116563855097025\n",
      "0.21001870422156949\n",
      "0.20843876165746017\n",
      "0.20691325496206306\n",
      "0.20543913803082542\n",
      "0.20401359655053725\n",
      "0.2026340259274821\n",
      "0.20129801171836953\n",
      "0.20000331223698317\n",
      "0.1987478430576132\n"
     ]
    }
   ],
   "source": [
    "for loopyloop in range(1000):\n",
    "    for i in range(m):    \n",
    "        z = 0\n",
    "        for j in range(n):\n",
    "            z += W[j]*X[i,j]\n",
    "            z += b\n",
    "        a = 1 / (1 + math.exp(-z))\n",
    "        L = -(Y[i]*math.log(a) + (1-Y[i])*math.log(1-a))\n",
    "        E += L\n",
    "\n",
    "\n",
    "        dz = a-Y[i]\n",
    "        for j in range(n):\n",
    "            dW[j] += X[i,j]*dz\n",
    "\n",
    "        db += dz\n",
    "\n",
    "    db = db/m\n",
    "    dW = [dw/m for dw in dW]\n",
    "    E = E/m\n",
    "\n",
    "    for w in range(len(W)):\n",
    "        W[w] -= (dW[w])\n",
    "    b -= db\n",
    "    \n",
    "    if loopyloop%20 == 0:\n",
    "        print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.432921336272889e-05\n",
      "[1.9962283174205204e-05, 2.477167713284318e-05]\n",
      "[0.07879016806161626, -0.12630085216799286]\n",
      "-5.046993099574763\n"
     ]
    }
   ],
   "source": [
    "print(E)\n",
    "print(dW)\n",
    "print(W)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
